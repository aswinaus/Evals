{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aswinaus/Evals/blob/main/RAG_with_Evaluation_RAGAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvsKbF7BJ3pw"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain_community langchain_openai chromadb pymupdf nest_asyncio --quiet\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough\n",
        ")\n",
        "from langchain.schema.output_parser import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the nest_asyncio library. This library provides a way to run asyncio code within an existing event loop, avoiding conflicts.\n",
        "\n",
        "nest_asyncio.apply(): The core it \"patches\" the asyncio event loop to allow it to run inside the existing event loop of your environment. In simpler terms, it makes sure your asynchronous code plays nicely within the notebook environment without causing errors.\n",
        "\n",
        "import os:  Line imports the os library, which is a standard Python library for interacting with the operating system."
      ],
      "metadata": {
        "id": "UjI9RblpbsmG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPfx1M_OJ6y3"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import os\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Ksz53v-J9M8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LBLYjXzKBFI"
      },
      "outputs": [],
      "source": [
        "import pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAwTNAkKKbOn"
      },
      "outputs": [],
      "source": [
        "# Download Data\n",
        "data_dir = '/content/drive/MyDrive' # Input a data dir path from your mounted Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Br6XhDCJKgQc"
      },
      "outputs": [],
      "source": [
        "doc = pymupdf.open(f\"{data_dir}/RAG/data/TP/Intel_Financial_Statements_Year_Ended_2017.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4InqskVKjfK"
      },
      "outputs": [],
      "source": [
        "#Printing the content to validate\n",
        "for page in doc:\n",
        "    text = page.get_text()\n",
        "    #print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMLNbDNBK4kz"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from langchain.embeddings import OpenAIEmbeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQDpOg7XLWGW"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "pages=[]\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "loader = PyMuPDFLoader(f\"{data_dir}/RAG/data/TP/Intel_Financial_Statements_Year_Ended_2017.pdf\")\n",
        "# load_and_split uses RecursiveCharacterTextSplitter by default\n",
        "pages_to_persist = loader.load_and_split(text_splitter)\n",
        "pages.extend(pages_to_persist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROttjL9nkGaD"
      },
      "outputs": [],
      "source": [
        "# split the pages into chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "splits = text_splitter.split_documents(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VVQbIZ5NT0k"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeiQrf3xNY9z"
      },
      "outputs": [],
      "source": [
        "# create vector store with Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.vectorstores.utils import filter_complex_metadata # import filter_complex_metadata\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=pages, embedding=OpenAIEmbeddings(openai_api_key=os.environ[\"OPENAI_API_KEY\"]),persist_directory=f\"{data_dir}/RAG/VectorDB/chroma_db_RAG\")\n",
        "vectordb.persist()\n",
        "retriever = vectordb.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asakQ43Vs4em"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RunnablePassthrough.assign():** This function is a core part of LangChain, a framework for building language model applications. RunnablePassthrough is like a pipe that allows data to flow through while potentially modifying or adding to it. .assign() is used here to add a new key-value pair to the data being passed through.\n",
        "\n",
        "**context= :** This part specifies that the key we are adding is called context. The value associated with this key is determined by the expression on the right side of the equals sign. This context will hold the relevant information retrieved from the document.\n",
        "\n",
        "**lambda x: :** This is an anonymous function (also called a lambda function) in Python. It takes one input (x, which will be a dictionary containing the user's question) and performs an operation to produce an output. This output becomes the value of the context key.\n",
        "\n",
        "**vectordb.similarity_search(x[\"question\"], k=10):** This is where the magic happens.\n",
        "\n",
        "**vectordb** is a Chroma vector database containing the embeddings of the document you loaded earlier (Intel Financial Statements).\n",
        "similarity_search is a method that searches the vector database for the documents most similar to a given query.\n",
        "\n",
        "**x[\"question\"]** provides the user's question as the query.\n",
        "\n",
        "**k=10** specifies that we want to retrieve the top 10 most similar documents."
      ],
      "metadata": {
        "id": "ah-ri7gBckKk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgTuW3rXgvND"
      },
      "outputs": [],
      "source": [
        "#Creating a RAG Pipeline\n",
        "from operator import itemgetter\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "# RAG\n",
        "template = \"\"\"You are an AI language model Accounting assistant.Answer the following question based on this context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "llm = ChatOpenAI(temperature=0, openai_api_key=os.environ[\"OPENAI_API_KEY\"])\n",
        "final_rag_chain = (\n",
        "    #{\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "\n",
        "    RunnablePassthrough.assign(\n",
        "        context=lambda x: format_docs(vectordb.similarity_search(x[\"question\"], k=10)),\n",
        "    )\n",
        "\n",
        "    #| RunnablePassthrough.assign(debug_context=lambda x: print(f\"Context before prompt: {x['context']}\"))\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMYzkbeQz1-o"
      },
      "outputs": [],
      "source": [
        "question=\"Can you let me know the Identified intangible assets subject to amortization and show the difference between 2016 and 2017?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "raZD7rnKtWMl"
      },
      "outputs": [],
      "source": [
        "final_rag_chain.invoke({\"question\":question})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTDUaovL6co4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AeeBYD2Sof_"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"Can you get the total amount of Goodwill and Identified Intangible Assets?\",\n",
        "    \"How much did Intangibles such as Goodwill and other identified intangible assets did Intel gain by acquiring Altera in millions?\",\n",
        "    \"Can you list all the Intel Goodwill activities for year 2017 along with figures in millions?\",\n",
        "    \"Can let me know how much was spent on Data Center Group along for 2016 and 2017 and show the difference between 2016 and 2017?\",\n",
        "    \"Can you let me know the Identified intangible assets subject to amortization and show the difference between 2016 and 2017?\",\n",
        "    ]\n",
        "ground_truth = [\n",
        "    \"The total amount of Goodwill is $10,278 million, and the total amount of Identified Intangible Assets is $7,566 million.\",\n",
        "    \"Intel gained $13,014 million in intangibles such as Goodwill and other identified intangible assets by acquiring Altera.\",\n",
        "    \"Sure, here are the Intel Goodwill activities for the year 2017 along with figures in millions:- Client Computing Group: $4,356;- Data Center Group: $5,421;- Internet of Things Group: $1,126;- Programmable Solutions Group: $2,490;- All other: $10,996;Total: $24,389 million\",\n",
        "    \"In 2016, the amount spent on the Data Center Group was $7,520 million, and in 2017, it was $8,395 million. The difference between the two years is $875 million, with an increase in spending on the Data Center Group from 2016 to 2017.\",\n",
        "    \"The Identified intangible assets subject to amortization for 2016 were $8,686 million, and for 2017, they were $10,577 million. The difference between 2016 and 2017 is $1,891 million.\",\n",
        "    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAkUWnEAKb4-"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heprVnxfxikf"
      },
      "outputs": [],
      "source": [
        "!pip install datasets --quiet\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJnwuQ3qHVb7"
      },
      "outputs": [],
      "source": [
        "answers  = []\n",
        "contexts = []\n",
        "\n",
        "# traversing each question and passing into the chain to get answer from the system\n",
        "for question in questions:\n",
        "    answers.append(final_rag_chain.invoke({\"question\":question}))\n",
        "    contexts.append([docs.page_content for docs in retriever.get_relevant_documents(question)])\n",
        "\n",
        "# Preparing the dataset\n",
        "data = {\n",
        "    \"question\": questions,\n",
        "    \"answer\": answers,\n",
        "    \"contexts\": contexts,\n",
        "    \"ground_truth\": ground_truth\n",
        "}\n",
        "\n",
        "# Convert dict to dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "dataset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bInjb2pxtwBb"
      },
      "outputs": [],
      "source": [
        "!pip install ragas --quiet\n",
        "import ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYneyjx9zVDo"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/aswinaus/rag_dataset_ragas.git\n",
        "#%cd rag_dataset_ragas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmyL7E3p7acZ"
      },
      "outputs": [],
      "source": [
        "#from datasets import load_dataset\n",
        "#dataset = load_dataset('json', data_files='RAGDataset.json')\n",
        "#dataset = dataset['train']\n",
        "#print(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code is focused on evaluating the performance of a Retrieval Augmented Generation (RAG) system using the ragas library. RAG systems combine information retrieval (finding relevant documents) with text generation (creating answers).\n",
        "\n",
        "**from ragas import evaluate:** This line imports the evaluate function from the ragas library. This function is the main tool for assessing the RAG system's quality.\n",
        "\n",
        "**from ragas.metrics import (...):** Here, specific evaluation metrics are imported from ragas.metrics. These metrics will be used to judge different aspects of the system's performance.\n",
        "\n",
        "**faithfulness:** Measures how well the generated answer aligns with the information provided in the retrieved documents. It checks if the answer is supported by the evidence.\n",
        "\n",
        "**answer_relevancy:** Assesses the relevance of the generated answer to the user's question. It determines if the answer addresses the question appropriately.\n",
        "\n",
        "**context_recall:** Evaluates how well the system retrieves all the necessary documents relevant to the question. A higher recall means more relevant documents are found.\n",
        "\n",
        "**context_precision:** Measures the accuracy of the retrieved documents. A higher precision means that a larger proportion of the retrieved documents are actually relevant."
      ],
      "metadata": {
        "id": "GxriZI3idob4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBtwVZQOHctE"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        context_precision,\n",
        "        context_recall,\n",
        "        faithfulness,\n",
        "        answer_relevancy,\n",
        "    ],\n",
        ")\n",
        "\n",
        "df = result.to_pandas()\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMocfZM6DKusN003up2oxQo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}